{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e52af489",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "\n",
    "<h2>A2: Kaggle Ranking - SF - TEAM_1</h2>\n",
    "<h4>Business Challenge #2 - BCH-7812 - FMBAN1</h4>\n",
    "<br>\n",
    "Professor - Chase Kusterer<br>\n",
    "Hult International Business School<br>\n",
    "<h4>Team Members</h4>\n",
    "<br>\n",
    "Dennis Otieno\n",
    "<br>\n",
    "Rhea Kapoor\n",
    "<br>\n",
    "Ofonimo Ben\n",
    "<br>\n",
    "Kevin Kabore\n",
    "<br>\n",
    "Htet Aung Kyaw\n",
    "\n",
    "<br>\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10dd1f27",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-06T21:50:57.575205Z",
     "iopub.status.busy": "2023-03-06T21:50:57.573920Z",
     "iopub.status.idle": "2023-03-06T21:50:57.591930Z",
     "shell.execute_reply": "2023-03-06T21:50:57.590412Z"
    },
    "papermill": {
     "duration": 0.029953,
     "end_time": "2023-03-06T21:50:57.596086",
     "exception": false,
     "start_time": "2023-03-06T21:50:57.566133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing critical libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pandas                      as pd             # data science essentials\n",
    "import matplotlib.pyplot           as plt            # essential graphical output\n",
    "import seaborn                     as sns            # enhanced graphical output\n",
    "import statsmodels.formula.api     as smf            # regression modeling\n",
    "import numpy                       as np             # mathematical essentials\n",
    "import sklearn.linear_model                          # linear modeling in scikit-learn\n",
    "from sklearn.model_selection import train_test_split # train/test split\n",
    "from sklearn.tree import plot_tree                   # tree plots\n",
    "from sklearn.model_selection import RandomizedSearchCV # hyperparameter tuning\n",
    "from sklearn.ensemble import GradientBoostingRegressor # gbm\n",
    "from sklearn.linear_model import LogisticRegression  #logistic regression\n",
    "from sklearn.ensemble import RandomForestClassifier  #Random Forest\n",
    "from sklearn.metrics import roc_auc_score            #roc_auc_score scoring\n",
    "from sklearn.metrics import confusion_matrix         #confusion matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier   # KNN for classification\n",
    "from sklearn.neighbors import KNeighborsRegressor    # KNN for regression\n",
    "from sklearn.preprocessing import StandardScaler     # standard scaler\n",
    "from sklearn.tree import DecisionTreeClassifier      # classification trees\n",
    "from sklearn.ensemble import GradientBoostingClassifier #gradient boosting\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e95158c",
   "metadata": {},
   "source": [
    "Let's Import and read the datasets from the train and test excel files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec796e1",
   "metadata": {
    "cell_style": "split",
    "execution": {
     "iopub.execute_input": "2023-03-06T21:50:59.193182Z",
     "iopub.status.busy": "2023-03-06T21:50:59.192702Z",
     "iopub.status.idle": "2023-03-06T21:50:59.323363Z",
     "shell.execute_reply": "2023-03-06T21:50:59.321531Z"
    },
    "papermill": {
     "duration": 0.141251,
     "end_time": "2023-03-06T21:50:59.325738",
     "exception": true,
     "start_time": "2023-03-06T21:50:59.184487",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8693 entries, 0 to 8692\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   PassengerId   8693 non-null   object \n",
      " 1   HomePlanet    8492 non-null   object \n",
      " 2   CryoSleep     8476 non-null   object \n",
      " 3   Cabin         8494 non-null   object \n",
      " 4   Destination   8511 non-null   object \n",
      " 5   Age           8514 non-null   float64\n",
      " 6   VIP           8490 non-null   object \n",
      " 7   RoomService   8512 non-null   float64\n",
      " 8   FoodCourt     8510 non-null   float64\n",
      " 9   ShoppingMall  8485 non-null   float64\n",
      " 10  Spa           8510 non-null   float64\n",
      " 11  VRDeck        8505 non-null   float64\n",
      " 12  Name          8493 non-null   object \n",
      " 13  Transported   8693 non-null   bool   \n",
      "dtypes: bool(1), float64(6), object(7)\n",
      "memory usage: 891.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# importing the training dataset\n",
    "path             = \"./\"\n",
    "training_dataset = \"train.csv\"\n",
    "\n",
    "# reading in the .csv file with pandas\n",
    "titanic_train    = pd.read_csv(filepath_or_buffer = path + training_dataset)\n",
    "\n",
    "# checking basic info about the dataset\n",
    "titanic_train.info(verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "778520ab",
   "metadata": {
    "cell_style": "split",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4277 entries, 0 to 4276\n",
      "Data columns (total 13 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   PassengerId   4277 non-null   object \n",
      " 1   HomePlanet    4190 non-null   object \n",
      " 2   CryoSleep     4184 non-null   object \n",
      " 3   Cabin         4177 non-null   object \n",
      " 4   Destination   4185 non-null   object \n",
      " 5   Age           4186 non-null   float64\n",
      " 6   VIP           4184 non-null   object \n",
      " 7   RoomService   4195 non-null   float64\n",
      " 8   FoodCourt     4171 non-null   float64\n",
      " 9   ShoppingMall  4179 non-null   float64\n",
      " 10  Spa           4176 non-null   float64\n",
      " 11  VRDeck        4197 non-null   float64\n",
      " 12  Name          4183 non-null   object \n",
      "dtypes: float64(6), object(7)\n",
      "memory usage: 434.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# importing the training dataset\n",
    "path             = \"./\"\n",
    "testing_dataset  = 'test.csv'\n",
    "\n",
    "# importing the testing dataset\n",
    "titanic_test = pd.read_csv(filepath_or_buffer = path + testing_dataset)\n",
    "\n",
    "# checking basic info about the dataset\n",
    "titanic_test.info(verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6062e9f2",
   "metadata": {},
   "source": [
    "<h2>Joining The Data and Handling Missing Values</h2>\n",
    "<br>\n",
    "<br>\n",
    "We join the data so that we can get a good picture of the whole dataset.\n",
    "<br>\n",
    "<br>\n",
    "Looking for the columns that have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a3da384",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\domuy\\AppData\\Local\\Temp\\ipykernel_170664\\1196545245.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  titanic_df = titanic_train.append(other = titanic_test)\n"
     ]
    }
   ],
   "source": [
    "#Setting the colums for joining the two datasets\n",
    "titanic_train['set'] = 'Training'\n",
    "titanic_test ['set'] = 'Testing'\n",
    "\n",
    "# concatenating both datasets together for mv and feature engineering\n",
    "titanic_df = titanic_train.append(other = titanic_test)\n",
    "\n",
    "# resetting index to avoid problems later in the code\n",
    "titanic_df.reset_index(drop = False,\n",
    "                       inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e3fb5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index              0\n",
       "PassengerId        0\n",
       "HomePlanet       288\n",
       "CryoSleep        310\n",
       "Cabin            299\n",
       "Destination      274\n",
       "Age              270\n",
       "VIP              296\n",
       "RoomService      263\n",
       "FoodCourt        289\n",
       "ShoppingMall     306\n",
       "Spa              284\n",
       "VRDeck           268\n",
       "Name             294\n",
       "Transported     4277\n",
       "set                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52204ced",
   "metadata": {},
   "source": [
    "Based on the data and columns that have missing values, the following will be done to fill the columns with missing values:\n",
    "<br>\n",
    "<br>\n",
    "1. Home planet will be filled with unknown because it is a string.\n",
    "<br>\n",
    "<br>\n",
    "2. CryoSleep will be design in a binary categorical nature to flag out the missing values.\n",
    "<br>\n",
    "<br>\n",
    "3. Cabin will be filled with UNKNOWN/2000/UNKNOWN beacuse the parts for deck and side are objects but the num is an integer\n",
    "<br>\n",
    "<br>\n",
    "4. Destination will be filled with unknown because it is a string.\n",
    "<br>\n",
    "<br>\n",
    "5. Name will be filled with unknown because it is a string.\n",
    "<br>\n",
    "<br>\n",
    "6. Age will be filled with the mean 28.\n",
    "<br>\n",
    "<br>\n",
    "7. VIP will be design in a binary categorical nature to flag out the missing values.\n",
    "<br>\n",
    "<br>\n",
    "8. The continuous columns will be filled with -999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b5902c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling the missing value in HomePlane with unknown\n",
    "titanic_df.HomePlanet.fillna(\"UKNOWN\", inplace= True)\n",
    "#filling the missing value in name with unknown\n",
    "titanic_df.Name.fillna(\"UKNOWN\", inplace=True)\n",
    "#filling the missing value in VRDeck with -999\n",
    "titanic_df.VRDeck.fillna(-999, inplace= True)\n",
    "#filling the missing value in Spa with -999\n",
    "titanic_df.Spa.fillna(-999, inplace= True)\n",
    "#filling the missing value in ShoppingMall with -999\n",
    "titanic_df.ShoppingMall.fillna(-999, inplace=True)\n",
    "#filling the missing value in FoodCourt with -999\n",
    "titanic_df.FoodCourt.fillna(-999, inplace=True)\n",
    "#filling the missing value in RoomService with -999\n",
    "titanic_df.RoomService.fillna(-999, inplace=True)\n",
    "#filling the missing value in Age with average 28\n",
    "titanic_df.Age.fillna(28, inplace=True)\n",
    "#filling the missing value in cabin with unknown/2000/2000\n",
    "titanic_df.Cabin.fillna(f\"UKNOWN/{2000}/UNKNOWN\", inplace= True)\n",
    "#creating new binary column for Cryosleep_true\n",
    "titanic_df[\"CryoSleep_TRUE\"] = 0\n",
    "#creating new binary column for Cryosleep_false\n",
    "titanic_df[\"CryoSleep_False\"] = 0\n",
    "#creating new binary column for vip_true\n",
    "titanic_df[\"VIP_TRUE\"] = 0\n",
    "#creating new binary column for vip_true\n",
    "titanic_df[\"VIP_FALSE\"] = 0\n",
    "# creating new columns\n",
    "titanic_df[\"Surname\"] = titanic_df[\"Name\"].copy()\n",
    "titanic_df[\"GroupId\"] = titanic_df['PassengerId'].copy()\n",
    "titanic_df[\"MemberId\"] = titanic_df[\"PassengerId\"].copy()\n",
    "#for loop to look for all the data considering our condition\n",
    "for index, values in titanic_df.iterrows():\n",
    "    #condition for creating a binary column cryosleep true\n",
    "    if titanic_df.loc[index,\"CryoSleep\"] == True:\n",
    "        titanic_df.loc[index,'CryoSleep_TRUE'] = 1\n",
    "    #condition for creating a binary column cryosleep false    \n",
    "    if titanic_df.loc[index,\"CryoSleep\"] == False:\n",
    "        titanic_df.loc[index,'CryoSleep_False'] = 1\n",
    "    #condition for creating a binary column vip true    \n",
    "    if titanic_df.loc[index, \"VIP\"] == True:\n",
    "        titanic_df.loc[index, 'VIP_TRUE'] = 1\n",
    "    #condition for creating a binary column vip true    \n",
    "    if titanic_df.loc[index, \"VIP\"] == False:\n",
    "        titanic_df.loc[index, 'VIP_FALSE'] = 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2163b4f8",
   "metadata": {},
   "source": [
    "<h2>Feature Engineering</h2>\n",
    "<br>\n",
    "<br>\n",
    "The Home Planet feature is being split into three separate boolean variables for Earth, Europa, and Mars. If a passenger is from Earth, then the Earth variable will be set to 1 and the Mars and Europa variables will be set to 0. If a passenger is from Europa or Mars, then their respective variables will be set to 1 and the Earth variable will be set to 0.\n",
    "<br>\n",
    "<br>\n",
    "The CyroSleep variable is being transformed into a binary variable where it takes the value 1 if the passenger was sleeping during the trip and 0 if the passenger was awake. \n",
    "<br>\n",
    "<br>\n",
    "The Cabin feature contains multiple pieces of information in the form of deck/num/side. To simplify the feature, the decision was made to store each piece of information in a separate feature. The Deck information is being split into eight different features, each corresponding to a different deck: 'Deck_A', 'Deck_B', 'Deck_C', 'Deck_D', 'Deck_E', 'Deck_F', 'Deck_G', 'Deck_T'. If a passenger was residing in a particular deck, then the corresponding feature for that deck will be set to 1, and all other deck features will be set to 0.\n",
    "<br>\n",
    "<br>\n",
    "The Home_Planet to Destination features (Earth_To_Cancries, Earth_To_TRAPPIST, Earth_To_PSO, Mars_To_TRAPPIST, Mars_To_Cancries, Mars_To_PSO, Europa_To_TRAPPIST, Europa_To_PSO, Europa_To_Cancries ) is a combination of two separate features that show where the passenger comes from (Home_Planet) and where they are going (Destination). The feature is set to 1 if the passenger if traveling from is Home planet to the right destination.\n",
    "<br>\n",
    "<br>\n",
    "For the Destination feature, the decision was made to create a binary column for each destination that will be set to 1 if the passenger is traveling in that direction.\n",
    "<br>\n",
    "<br>\n",
    "We then transformed the VIP column into two variables, VIP_True and VIP_False. VIP_True takes the value 1 if the passenger is a VIP, and 0 otherwise. Similarly, VIP_False takes the value 1 if the passenger is not a VIP, and 0 otherwise.\n",
    "<br>\n",
    "<br>\n",
    "To create the Not_Alone feature that identifies passengers who are traveling in groups, we used the passenger_id column, which contains the number of people within each group. And when that number was above 1 we classify that passanger as Not_Alone \n",
    "<br>\n",
    "<br>\n",
    "The Total Spending feature is the sum of each spending on luxury amenities: RoomService, FoodCourt, ShoppingMall, Spa, and VRDeck. The Wealth_Person feature is built based on the average Total Spending value. Any passenger whose Total Spending value is above the average is considered wealthy.\n",
    "<br>\n",
    "<br>\n",
    "It was observed that the RoomService, FoodCourt, ShoppingMall, Spa, and VRDeck features contained many 0 values, which may represent the fact that the passenger did not purchase these services. Therefore, new binary features were created, including With_RoomService, With_FoodCourt, With_Spa, With_VRDeck, and With_ShoppingMall, which take the value 1 if the passenger spent any amount on the corresponding service and 0 otherwise.\n",
    "<br>\n",
    "<br>\n",
    "We then decided to divide the age category into four different categories. Children include every passenger below 13 years old, Youth includes every passenger from 13 to 35 years old, Adults include every passenger from 35 to 65 years old, and Elder includes every passenger above 65 years old.\n",
    "<br>\n",
    "<br>\n",
    "The Same_Surname feature is binary and is built using passengers who share a similar last name to group people who might be part of the same family. It is set to 1 when the last name of the passenger matches with one or more passengers.\n",
    "<br>\n",
    "<br>\n",
    "Cabin_headcount gives us the average number of people in each cabin who travel with More_than_one_pax, which is a binary feature that shows when a cabin has more than an average of 2 people in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c846ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating binary for the column transported\n",
    "titanic_df[\"Transported_New\"] = 0\n",
    "#creating binary for the Home Planet\n",
    "titanic_df[\"Earth\"] = 0\n",
    "titanic_df[\"Europa\"] = 0\n",
    "titanic_df[\"Mars\"] = 0\n",
    "#creating binary for the Destination column\n",
    "titanic_df[\"TRAPPIST\"] = 0\n",
    "titanic_df[\"Cancri\"] = 0\n",
    "titanic_df[\"PSO\"] = 0\n",
    "#creating the new columns from the split column cabin\n",
    "titanic_df[\"Deck\"] = titanic_df['Cabin'].copy()\n",
    "titanic_df.Deck.fillna(f\"UKNOWN/{2000}/UNKNOWN\", inplace= True)\n",
    "titanic_df[\"Num\"] = titanic_df['Cabin'].copy()\n",
    "titanic_df.Num.fillna(f\"UKNOWN/{2000}/UNKNOWN\", inplace= True)\n",
    "titanic_df[\"Side\"] = titanic_df['Cabin'].copy()\n",
    "titanic_df.Side.fillna(f\"UKNOWN/{2000}/UNKNOWN\", inplace= True)\n",
    "#creating another column total lux spending for all the continuous variables\n",
    "titanic_df['Total_Lux_Spending'] = titanic_df['RoomService'] + titanic_df['FoodCourt'] + titanic_df['ShoppingMall'] + titanic_df['Spa'] + titanic_df['VRDeck']\n",
    "#creating binary columns for the continuous variables to cater for the zeros\n",
    "titanic_df[\"With_Family\"] = 0\n",
    "titanic_df[\"With_RoomService\"] = 0\n",
    "titanic_df[\"With_FoodCourt\"] = 0\n",
    "titanic_df[\"With_Spa\"] = 0\n",
    "titanic_df[\"With_VRDeck\"] = 0\n",
    "titanic_df[\"With_ShoppingMall\"] = 0\n",
    "# creating new columns for the age brackets\n",
    "titanic_df['Teenagers'] = 0\n",
    "titanic_df['Youth'] = 0\n",
    "titanic_df['Adults'] = 0\n",
    "titanic_df['Seniors'] = 0\n",
    "titanic_df['Children'] = 0\n",
    "#creating new columns for passengers from a home planet to specific destination\n",
    "titanic_df[\"Earth_To_TRAPPIST\"] = 0\n",
    "titanic_df[\"Earth_To_Cancrie\"] = 0\n",
    "titanic_df[\"Earth_To_PSO\"] = 0\n",
    "titanic_df[\"Europa_To_TRAPPIST\"] = 0\n",
    "titanic_df[\"Europa_To_Cancrie\"] = 0\n",
    "titanic_df[\"Europa_To_PSO\"] = 0\n",
    "#creating new columns for passengers from a home planet to specific destination\n",
    "titanic_df[\"Mars_To_TRAPPIST\"] = 0\n",
    "titanic_df[\"Mars_To_Cancrie\"] = 0\n",
    "titanic_df[\"Mars_To_PSO\"] = 0\n",
    "#passengers who have family \n",
    "titanic_df[\"Not_Alone\"] = 0\n",
    "#creating a column for wealthy person\n",
    "titanic_df[\"Wealthy_Person\"] = 0\n",
    "#creating a new column for passengers with the same surname\n",
    "titanic_df[\"Same_Surname\"] = 0\n",
    "#passengers per cabin \n",
    "titanic_df[\"Cabin_Headcount\"] = 0\n",
    "#passengers from earth and mars as humans and non_humans\n",
    "titanic_df[\"Earth_and_Mars\"] = 0\n",
    "titanic_df[\"More_than_one_pax\"] = 0\n",
    "#accounting for ever deck in the ship\n",
    "titanic_df[\"Deck_A\"] = 0\n",
    "titanic_df[\"Deck_B\"] = 0\n",
    "titanic_df[\"Deck_C\"] = 0\n",
    "titanic_df[\"Deck_D\"] = 0\n",
    "titanic_df[\"Deck_E\"] = 0\n",
    "#accounting for every deck in the ship\n",
    "titanic_df[\"Deck_F\"] = 0\n",
    "titanic_df[\"Deck_G\"] = 0\n",
    "titanic_df[\"Deck_T\"] = 0\n",
    "#accounting for every side in the ship\n",
    "titanic_df[\"Side_P\"] = 0\n",
    "titanic_df[\"Side_S\"] = 0\n",
    "for index, values in titanic_df.iterrows():\n",
    "    #condition for creating binary for the column transported\n",
    "    if titanic_df.loc[index,\"Transported\"] == True:\n",
    "        titanic_df.loc[index,\"Transported_New\"] = 1\n",
    "    if titanic_df.loc[index,\"Transported\"] == False:\n",
    "        titanic_df.loc[index,\"Transported_New\"] = 0\n",
    "    #condition for creating binary for the Home Planet    \n",
    "    if titanic_df.loc[index,\"HomePlanet\"] == \"Earth\":\n",
    "         titanic_df.loc[index,\"Earth\"] = 1\n",
    "    if titanic_df.loc[index,\"HomePlanet\"] == \"Europa\":\n",
    "         titanic_df.loc[index,\"Europa\"] = 1\n",
    "    if titanic_df.loc[index,\"HomePlanet\"] == \"Mars\":\n",
    "         titanic_df.loc[index,\"Mars\"] = 1\n",
    "    #condition for creating binary for the Destination column        \n",
    "    if titanic_df.loc[index,\"Destination\"] == \"TRAPPIST-1e\":\n",
    "        titanic_df.loc[index,\"TRAPPIST\"] = 1\n",
    "    if titanic_df.loc[index,\"Destination\"] == \"55 Cancri e\":\n",
    "        titanic_df.loc[index,\"Cancri\"] = 1\n",
    "    if titanic_df.loc[index,\"Destination\"] == \"PSO J318.5-22\":\n",
    "        titanic_df.loc[index,\"PSO\"] = 1 \n",
    "    #creating the new columns from the split column cabin    \n",
    "    titanic_df.loc[index,\"Deck\"] = titanic_df.loc[index,\"Deck\"].split(f\"/\")[0]\n",
    "    titanic_df.loc[index,\"Num\"] = titanic_df.loc[index,\"Num\"].split(f\"/\")[1]\n",
    "    titanic_df.loc[index,\"Side\"] = titanic_df.loc[index,\"Side\"].split(f\"/\")[2] \n",
    "    \n",
    "    #creating binary columns for the continuous variables to cater for the zeros\n",
    "    if titanic_df.loc[index,\"RoomService\"] != 0 :\n",
    "        titanic_df.loc[index, \"With_RoomService\"] = 1\n",
    "\n",
    "    if titanic_df.loc[index,\"FoodCourt\"] != 0 :\n",
    "        titanic_df.loc[index, \"With_FoodCourt\"] = 1\n",
    "\n",
    "    if titanic_df.loc[index,\"Spa\"] != 0 :\n",
    "        titanic_df.loc[index, \"With_Spa\"] = 1\n",
    "    #creating binary columns for the continuous variables to cater for the zeros\n",
    "    if titanic_df.loc[index,\"VRDeck\"] != 0 :\n",
    "        titanic_df.loc[index, \"With_VRDeck\"] = 1\n",
    "\n",
    "    if titanic_df.loc[index,\"ShoppingMall\"] != 0 :\n",
    "        titanic_df.loc[index, \"With_ShoppingMall\"] = 1\n",
    "     \n",
    "    # creating new columns for the age brackets\n",
    "    if titanic_df.loc[index, 'Age'] < 13:\n",
    "        titanic_df.loc[index, 'Children'] = 1   \n",
    "    \n",
    "    elif titanic_df.loc[index, 'Age'] >= 13 and titanic_df.loc[index, 'Age'] < 20:\n",
    "        titanic_df.loc[index, 'Teenagers'] = 1 \n",
    "\n",
    "    elif titanic_df.loc[index, 'Age'] >= 20 and titanic_df.loc[index, 'Age'] < 35:\n",
    "        titanic_df.loc[index, 'Youth'] = 1 \n",
    "    # creating new columns for the age brackets\n",
    "    elif titanic_df.loc[index, 'Age'] >= 35 and titanic_df.loc[index, 'Age'] < 60:\n",
    "        titanic_df.loc[index, 'Adults'] = 1\n",
    "\n",
    "    elif titanic_df.loc[index, 'Age'] >= 60:\n",
    "        titanic_df.loc[index, 'Seniors'] = 1\n",
    "      \n",
    "    try:\n",
    "        titanic_df.loc[index,\"Name\"] = titanic_df.loc[index,\"Name\"].split(\" \")[0]\n",
    "        titanic_df.loc[index,\"Surname\"] = titanic_df.loc[index,\"Surname\"].split(\" \")[1]\n",
    "    except:\n",
    "        titanic_df.loc[index,\"Name\"] = titanic_df.loc[index,\"Name\"]\n",
    "        titanic_df.loc[index,\"Surname\"] = titanic_df.loc[index,\"Surname\"]\n",
    "    #accounting for ever deck in the ship\n",
    "    if titanic_df.loc[index,\"Deck\"] == \"A\":\n",
    "        titanic_df.loc[index, \"Deck_A\"] = 1\n",
    "    elif titanic_df.loc[index,\"Deck\"] == \"B\":\n",
    "        titanic_df.loc[index, \"Deck_B\"] = 1\n",
    "    elif titanic_df.loc[index,\"Deck\"] == \"C\":\n",
    "        titanic_df.loc[index, \"Deck_C\"] = 1\n",
    "    #accounting for ever deck in the ship    \n",
    "    elif titanic_df.loc[index,\"Deck\"] == \"D\":\n",
    "        titanic_df.loc[index, \"Deck_D\"] = 1\n",
    "    elif titanic_df.loc[index,\"Deck\"] == \"E\":\n",
    "        titanic_df.loc[index, \"Deck_E\"] = 1\n",
    "    elif titanic_df.loc[index,\"Deck\"] == \"F\":    \n",
    "        titanic_df.loc[index, \"Deck_F\"] = 1\n",
    "    #accounting for ever deck in the ship    \n",
    "    elif titanic_df.loc[index,\"Deck\"] == \"G\":\n",
    "        titanic_df.loc[index, \"Deck_G\"] = 1\n",
    "    elif titanic_df.loc[index,\"Deck\"] == \"T\":\n",
    "        titanic_df.loc[index, \"Deck_T\"] = 1\n",
    "        \n",
    "    #creating new columns for passengers from a home planet to specific destination  \n",
    "    if titanic_df.loc[index,\"HomePlanet\"] == \"Earth\" and \\\n",
    "        titanic_df.loc[index,\"Destination\"] == \"TRAPPIST-1e\":\n",
    "        titanic_df.loc[index, \"Earth_To_TRAPPIST\"] = 1\n",
    "    elif titanic_df.loc[index,\"HomePlanet\"] == \"Earth\" and \\\n",
    "        titanic_df.loc[index,\"Destination\"] == \"55 Cancri e\":\n",
    "        titanic_df.loc[index, \"Earth_To_Cancrie\"] = 1\n",
    "    elif titanic_df.loc[index,\"HomePlanet\"] == \"Earth\" and \\\n",
    "        titanic_df.loc[index,\"Destination\"] == \"PSO J318.5-22\":\n",
    "        titanic_df.loc[index, \"Earth_To_PSO\"] = 1\n",
    "    #creating new columns for passengers from a home planet to specific destination    \n",
    "    elif titanic_df.loc[index,\"HomePlanet\"] == \"Europa\" and \\\n",
    "        titanic_df.loc[index,\"Destination\"] == \"TRAPPIST-1e\":\n",
    "        titanic_df.loc[index, \"Europa_To_TRAPPIST\"] = 1\n",
    "    elif titanic_df.loc[index,\"HomePlanet\"] == \"Europa\" and \\\n",
    "        titanic_df.loc[index,\"Destination\"] == \"55 Cancri e\":\n",
    "        titanic_df.loc[index, \"Europa_To_Cancrie\"] = 1\n",
    "    elif titanic_df.loc[index,\"HomePlanet\"] == \"Europa\" and \\\n",
    "        titanic_df.loc[index,\"Destination\"] == \"PSO J318.5-22\":\n",
    "        titanic_df.loc[index, \"Europa_To_PSO\"] = 1\n",
    "    #creating new columns for passengers from a home planet to specific destination    \n",
    "    elif titanic_df.loc[index,\"HomePlanet\"] == \"Mars\" and \\\n",
    "        titanic_df.loc[index,\"Destination\"] == \"TRAPPIST-1e\":\n",
    "        titanic_df.loc[index, \"Mars_To_TRAPPIST\"] = 1\n",
    "    elif titanic_df.loc[index,\"HomePlanet\"] == \"Mars\" and \\\n",
    "        titanic_df.loc[index,\"Destination\"] == \"55 Cancri e\":\n",
    "        titanic_df.loc[index, \"Mars_To_Cancrie\"] = 1\n",
    "    elif titanic_df.loc[index,\"HomePlanet\"] == \"Mars\" and \\\n",
    "        titanic_df.loc[index,\"Destination\"] == \"PSO J318.5-22\":\n",
    "        titanic_df.loc[index, \"Mars_To_PSO\"] = 1\n",
    "    #accounting for every side in the ship   \n",
    "    if titanic_df.loc[index,\"Side\"] == \"S\":\n",
    "        titanic_df.loc[index, \"Side_S\"] = 1\n",
    "    elif titanic_df.loc[index,\"Side\"] == \"P\":\n",
    "        titanic_df.loc[index, \"Side_P\"] = 1\n",
    "    #passengers who have family    \n",
    "    if titanic_df.loc[index,\"MemberId\"] == 2:\n",
    "        titanic_df.loc[index, \"Not_Alone\"] = 1\n",
    "    elif titanic_df.loc[index,\"MemberId\"] == 3:\n",
    "        titanic_df.loc[index, \"Not_Alone\"] = 1\n",
    "    elif titanic_df.loc[index,\"MemberId\"] == 4:\n",
    "        titanic_df.loc[index, \"Not_Alone\"] = 1\n",
    "    #passengers who have family     \n",
    "    elif titanic_df.loc[index,\"MemberId\"] == 5:\n",
    "        titanic_df.loc[index, \"Not_Alone\"] = 1\n",
    "    elif titanic_df.loc[index,\"MemberId\"] == 6:\n",
    "        titanic_df.loc[index, \"Not_Alone\"] = 1\n",
    "    elif titanic_df.loc[index,\"MemberId\"] == 7:\n",
    "        titanic_df.loc[index, \"Not_Alone\"] = 1\n",
    "    #passengers who have family     \n",
    "    elif titanic_df.loc[index,\"MemberId\"] == 8:\n",
    "        titanic_df.loc[index, \"Not_Alone\"] = 1\n",
    "    #creating a column for wealthy person    \n",
    "    if titanic_df.loc[index,\"Total_Lux_Spending\"] >= 689  and \\\n",
    "        titanic_df.loc[index,\"CryoSleep\"] == False:\n",
    "        titanic_df.loc[index, \"Wealthy_Person\"] = 1\n",
    "            \n",
    "# count same Surname people\n",
    "name_counts = titanic_df['Surname'].value_counts().to_dict()\n",
    "# Create a new column 'Count' and map the name counts to each name\n",
    "titanic_df['Surname_Count'] = titanic_df['Surname'].map(name_counts)\n",
    "\n",
    "# count same Surname people\n",
    "head_counts = titanic_df['Cabin'].value_counts().to_dict()\n",
    "# Create a new column 'Count' and map the name counts to each name\n",
    "titanic_df['Cabin_Headcount'] = titanic_df['Cabin'].map(head_counts)\n",
    "\n",
    "for index, values in titanic_df.iterrows():\n",
    "    if titanic_df.loc[index,\"Surname_Count\"] != 1:\n",
    "        titanic_df.loc[index,'Same_Surname'] = 1\n",
    "#creating a loop for binary family column\n",
    "for index, values in titanic_df.iterrows():\n",
    "    if titanic_df.loc[index,\"MemberId\"] != 1 and \\\n",
    "        titanic_df.loc[index,\"Surname_Count\"] != 1:\n",
    "        titanic_df.loc[index,\"With_Family\"] = 1\n",
    "\n",
    "#passengers from earth and mars as humans and non_humans\n",
    "for index, values in titanic_df.iterrows():\n",
    "    if titanic_df.loc[index,\"Deck_D\"] == 1 or \\\n",
    "        titanic_df.loc[index,\"Deck_E\"] == 1 or \\\n",
    "        titanic_df.loc[index,\"Deck_F\"] == 1:\n",
    "        titanic_df.loc[index,\"Earth_and_Mars\"] = 1\n",
    "        \n",
    "    #creating a column with passengers who are not alone\n",
    "    if titanic_df.loc[index,\"Cabin_Headcount\"] >= 2:\n",
    "        titanic_df.loc[index,\"More_than_one_pax\"] = 1    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2571c0bc",
   "metadata": {},
   "source": [
    "<h2>CLASSIFICATION MODELING</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f0d9fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping some columns which are not important\n",
    "titanic_df.drop(columns=[\"HomePlanet\", \"CryoSleep\", \"Cabin\", \"Destination\", \"VIP\", \"Transported\", \"Deck\", \"Side\", \"Name\", \"Surname\" ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20272cf0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transported_New       1.00\n",
       "CryoSleep_TRUE        0.46\n",
       "CryoSleep_False       0.45\n",
       "Wealthy_Person        0.40\n",
       "With_RoomService      0.35\n",
       "With_Spa              0.34\n",
       "With_VRDeck           0.33\n",
       "With_ShoppingMall     0.26\n",
       "With_FoodCourt        0.23\n",
       "RoomService           0.23\n",
       "Spa                   0.22\n",
       "Total_Lux_Spending    0.20\n",
       "VRDeck                0.20\n",
       "Europa                0.18\n",
       "Earth_To_TRAPPIST     0.17\n",
       "Earth                 0.17\n",
       "Earth_and_Mars        0.16\n",
       "More_than_one_pax     0.15\n",
       "Deck_B                0.14\n",
       "Europa_To_Cancrie     0.13\n",
       "Children              0.13\n",
       "Cancri                0.11\n",
       "Deck_C                0.11\n",
       "Side_P                0.10\n",
       "Side_S                0.10\n",
       "Europa_To_TRAPPIST    0.10\n",
       "Deck_E                0.10\n",
       "Deck_F                0.09\n",
       "TRAPPIST              0.09\n",
       "Age                   0.07\n",
       "Youth                 0.07\n",
       "VIP_TRUE              0.04\n",
       "FoodCourt             0.04\n",
       "Mars_To_Cancrie       0.03\n",
       "With_Family           0.03\n",
       "Same_Surname          0.03\n",
       "Deck_D                0.03\n",
       "Deck_G                0.02\n",
       "index                 0.02\n",
       "Adults                0.02\n",
       "Europa_To_PSO         0.02\n",
       "Teenagers             0.02\n",
       "Mars                  0.02\n",
       "VIP_FALSE             0.02\n",
       "Mars_To_PSO           0.01\n",
       "Mars_To_TRAPPIST      0.01\n",
       "Seniors               0.01\n",
       "Deck_T                0.01\n",
       "ShoppingMall          0.01\n",
       "Cabin_Headcount       0.00\n",
       "Deck_A                0.00\n",
       "Earth_To_PSO          0.00\n",
       "Earth_To_Cancrie      0.00\n",
       "PSO                   0.00\n",
       "Surname_Count         0.00\n",
       "Not_Alone              NaN\n",
       "Name: Transported_New, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiating a correlation matrix\n",
    "titanic_corr = titanic_df[ titanic_df['set']   == 'Training' ].corr(method = 'pearson').round(decimals = 2)\n",
    "\n",
    "# transforming correlations to absolute values\n",
    "titanic_corr.loc[ : , 'Transported_New' ].apply(func = abs).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8943f8ac",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setting explanatory variable(s) with most correlated x-variable\n",
    "x_train = titanic_df[[\"CryoSleep_TRUE\",\n",
    "                      \"CryoSleep_False\",\n",
    "                      \"Wealthy_Person\",\n",
    "                      \"With_RoomService\",\n",
    "                      \"With_Spa\",\n",
    "                      \"With_VRDeck\",\n",
    "                      \"With_ShoppingMall\",\n",
    "                      \"RoomService\",\n",
    "                      \"Spa\",\n",
    "                      \"VRDeck\",\n",
    "                      \"Total_Lux_Spending\",\n",
    "                      \"Earth_To_TRAPPIST\",\n",
    "                      \"Deck_C\",\n",
    "                      \"Side_P\",\n",
    "                      \"Deck_E\",\n",
    "                      \"With_Family\",\n",
    "                      \"Mars\",\n",
    "                      \"Teenagers\",\n",
    "                      \"PSO\",\n",
    "                      \"Earth_To_Cancrie\",\n",
    "                      'More_than_one_pax'\n",
    "                      ]].copy()[ titanic_df['set'] == 'Training' ]\n",
    "\n",
    "# setting response variable\n",
    "y_train = titanic_df['Transported_New'][ titanic_df['set']   == 'Training' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e0b2260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CryoSleep_TRUE + \n",
      " CryoSleep_False + \n",
      " Wealthy_Person + \n",
      " With_RoomService + \n",
      " With_Spa + \n",
      " With_VRDeck + \n",
      " With_ShoppingMall + \n",
      " RoomService + \n",
      " Spa + \n",
      " VRDeck + \n",
      " Total_Lux_Spending + \n",
      " Earth_To_TRAPPIST + \n",
      " Deck_C + \n",
      " Side_P + \n",
      " Deck_E + \n",
      " With_Family + \n",
      " Mars + \n",
      " Teenagers + \n",
      " PSO + \n",
      " Earth_To_Cancrie + \n",
      " More_than_one_pax + \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for val in x_train:\n",
    "    print(f\" {val} + \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be780e40",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# developing training and validation sets\n",
    "x_train_1, x_train_2, y_train_1, y_train_2 = train_test_split(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            random_state = 123,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8862f504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Model Type       : Unpruned Gradient Boosting Classifier\n",
      "    Training ACCURACY: 0.8245\n",
      "    Testing ACCURACY : 0.8137\n",
      "    Train-Test Gap   : 0.0108\n",
      "    AUC Train        : 0.8244\n",
      "    AUC Test         : 0.8136\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# developing training and validation sets\n",
    "x_train_1, x_train_2, y_train_1, y_train_2 = train_test_split(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            random_state = 123,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = y_train)\n",
    "\n",
    "#Gradient Boosting\n",
    "# specifying a model name\n",
    "model_name = 'Unpruned Gradient Boosting Classifier'\n",
    "ugbc = GradientBoostingClassifier(loss          = 'exponential',\n",
    "                                    min_samples_split= 100,\n",
    "                                    min_samples_leaf= 103,\n",
    "                                    criterion     = 'squared_error',\n",
    "                                    n_estimators = 100,\n",
    "                                    max_depth     = 8,\n",
    "                                    random_state  = 123)\n",
    "\n",
    "# FITTING the training data\n",
    "ugbc_fit = ugbc.fit(x_train_1, y_train_1)\n",
    "\n",
    "\n",
    "# PREDICTING on the response variable\n",
    "ugbc_train_pred = ugbc_fit.predict(x_train_1)\n",
    "ugbc_valid_pred = ugbc_fit.predict(x_train_2)\n",
    "\n",
    "# # PREDICTING based on the testing set\n",
    "# ugbc_pred = ugbc.predict(x_test)\n",
    "\n",
    "# SCORING the results\n",
    "ugbc_train_score = ugbc.score(x_train_1, y_train_1).round(4) # using R-square\n",
    "ugbc_test_score  = ugbc.score(x_train_2, y_train_2).round(4)   # using R-square\n",
    "ugbc_gap         = abs(ugbc_train_score - ugbc_test_score).round(4)\n",
    "\n",
    "# SCORING the results (auc)\n",
    "ugbc_train_auc = roc_auc_score(y_true  = y_train_1,\n",
    "                                y_score = ugbc_train_pred).round(decimals = 4)\n",
    "\n",
    "ugbc_valid_auc = roc_auc_score(y_true  = y_train_2,\n",
    "                                y_score = ugbc_valid_pred).round(decimals = 4)\n",
    "\n",
    "print (f\"\"\"\n",
    "    Model Type       : {model_name}\n",
    "    Training ACCURACY: {ugbc_train_score}\n",
    "    Testing ACCURACY : {ugbc_test_score}\n",
    "    Train-Test Gap   : {ugbc_gap}\n",
    "    AUC Train        : {ugbc_train_auc}\n",
    "    AUC Test         : {ugbc_valid_auc}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38860ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fitting it in the train_set\n",
    "# x_train_1, x_train_2, y_train_1, y_train_2 = train_test_split(\n",
    "#             x_train,\n",
    "#             y_train,\n",
    "#             random_state = 123,\n",
    "#             test_size    = 0.25,\n",
    "#             stratify     = y_train)\n",
    "\n",
    "# #Gradient Boosting\n",
    "# # declaring a hyperparameter space\n",
    "# criterion_range             = ['friedman_mse', 'squared_error', 'mse']\n",
    "# loss_range                  = ['exponential', 'deviance']\n",
    "# depth_range                 = range(1, 9, 1)\n",
    "# leaf_range                  = range(1, 50, 1)\n",
    "# min_samples_split_range      = range(1,20,1)\n",
    "\n",
    "# # creating a hyperparameter grid\n",
    "# param_grid = {'criterion'       : criterion_range,\n",
    "#             'loss'              : loss_range,\n",
    "#             'max_depth'         : depth_range,\n",
    "#             'min_samples_leaf'  : leaf_range,\n",
    "#             'min_samples_split' : min_samples_split_range}\n",
    "\n",
    "\n",
    "# model_name = 'Pruned Gradient Boosting Classifier'\n",
    "\n",
    "# # INSTANTIATING a classification tree object\n",
    "# gradient_boosting = GradientBoostingClassifier(random_state = 219)\n",
    "\n",
    "# # RandomizedSearchCV object\n",
    "# gradient_boosting_cv = RandomizedSearchCV(estimator   = gradient_boosting,\n",
    "#                                 param_distributions   = param_grid,\n",
    "#                                 cv                    = 5,\n",
    "#                                 n_iter                = 10,\n",
    "#                                 random_state          = 123,\n",
    "#                                 scoring               = 'roc_auc')\n",
    "\n",
    "# # FITTING the training data\n",
    "# gradient_boosting_fit = gradient_boosting_cv.fit(x_train_1, y_train_1)\n",
    "\n",
    "# # PREDICTING on the response variable\n",
    "# model_train_pred = gradient_boosting_fit.predict(x_train_1)\n",
    "# model_valid_pred = gradient_boosting_fit.predict(x_train_2)\n",
    "\n",
    "# # saving scoring data for future use\n",
    "# gradient_boosting_train_score = gradient_boosting_fit.score(x_train_1, y_train_1).round(4) # accuracy\n",
    "\n",
    "# gradient_boosting_test_score  = gradient_boosting_fit.score(x_train_2, y_train_2).round(4)   # accuracy\n",
    "\n",
    "# gradient_boosting_model_gap   = abs(gradient_boosting_train_score - \n",
    "#                             gradient_boosting_test_score).round(4)\n",
    "\n",
    "# # saving AUC\n",
    "# gradient_boosting_train_auc_score   = roc_auc_score(y_true  = y_train_1,\n",
    "#                                     y_score = model_train_pred).round(4) # auc\n",
    "# gradient_boosting_test_auc_score   = roc_auc_score(y_true  = y_train_2,\n",
    "#                                     y_score = model_valid_pred).round(4) # auc\n",
    "\n",
    "# gradient_boosting_cv.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9edcb446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Model Type       : Unpruned Gradient Boosting Classifier\n",
      "    Training ACCURACY: 0.8425\n",
      "    Testing ACCURACY : 0.8036\n",
      "    Train-Test Gap   : 0.0389\n",
      "    AUC Train        : 0.8423\n",
      "    AUC Test         : 0.8034\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# developing training and validation sets\n",
    "x_train_1, x_train_2, y_train_1, y_train_2 = train_test_split(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            random_state = 123,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = y_train)\n",
    "\n",
    "#Gradient Boosting\n",
    "# specifying a model name\n",
    "model_name = 'Unpruned Gradient Boosting Classifier'\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "ugbc = GradientBoostingClassifier(loss          = 'exponential',\n",
    "                                    min_samples_split= 12,\n",
    "                                    min_samples_leaf= 44,\n",
    "                                    criterion     = 'squared_error',\n",
    "                                    max_depth     = 8,\n",
    "                                    random_state  = 123)\n",
    "\n",
    "# FITTING the training data\n",
    "ugbc_fit = ugbc.fit(x_train_1, y_train_1)\n",
    "\n",
    "\n",
    "# PREDICTING on the response variable\n",
    "ugbc_train_pred = ugbc_fit.predict(x_train_1)\n",
    "ugbc_valid_pred = ugbc_fit.predict(x_train_2)\n",
    "\n",
    "# # PREDICTING based on the testing set\n",
    "# ugbc_pred = ugbc.predict(x_test)\n",
    "\n",
    "# SCORING the results\n",
    "ugbc_train_score = ugbc.score(x_train_1, y_train_1).round(4) # using R-square\n",
    "ugbc_test_score  = ugbc.score(x_train_2, y_train_2).round(4)   # using R-square\n",
    "ugbc_gap         = abs(ugbc_train_score - ugbc_test_score).round(4)\n",
    "\n",
    "# SCORING the results (auc)\n",
    "ugbc_train_auc = roc_auc_score(y_true  = y_train_1,\n",
    "                                y_score = ugbc_train_pred).round(decimals = 4)\n",
    "\n",
    "ugbc_valid_auc = roc_auc_score(y_true  = y_train_2,\n",
    "                                y_score = ugbc_valid_pred).round(decimals = 4)\n",
    "\n",
    "print (f\"\"\"\n",
    "    Model Type       : {model_name}\n",
    "    Training ACCURACY: {ugbc_train_score}\n",
    "    Testing ACCURACY : {ugbc_test_score}\n",
    "    Train-Test Gap   : {ugbc_gap}\n",
    "    AUC Train        : {ugbc_train_auc}\n",
    "    AUC Test         : {ugbc_valid_auc}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad309513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = titanic_df[[\"CryoSleep_TRUE\",\n",
    "                      \"CryoSleep_False\",\n",
    "                      \"Wealthy_Person\",\n",
    "                      \"With_RoomService\",\n",
    "                      \"With_Spa\",\n",
    "                      \"With_VRDeck\",\n",
    "                      \"With_ShoppingMall\",\n",
    "                      \"RoomService\",\n",
    "                      \"Spa\",\n",
    "                      \"VRDeck\",\n",
    "                      \"Total_Lux_Spending\",\n",
    "                      \"Earth_To_TRAPPIST\",\n",
    "                      \"Deck_C\",\n",
    "                      \"Side_P\",\n",
    "                      \"Deck_E\",\n",
    "                      \"With_Family\",\n",
    "                      \"Mars\",\n",
    "                      \"Teenagers\",\n",
    "                      \"PSO\",\n",
    "                      \"Earth_To_Cancrie\",\n",
    "                      'More_than_one_pax'\n",
    "                      ]].copy()[ titanic_df['set'] == 'Testing' ]\n",
    "\n",
    "# # setting x_test\n",
    "# x_test  = titanic_df.drop(columns=[\"Transported_New\", \"Transported\", \"Name\", \"set\"], axis=1).copy()[ titanic_df['set'] == 'Testing' ]\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "model_pred = ugbc.predict(x_test)\n",
    "\n",
    "# checking results\n",
    "model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "414c8031",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0013_01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0018_01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0019_01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0021_01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0023_01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PassengerId  Transported\n",
       "0     0013_01         True\n",
       "1     0018_01        False\n",
       "2     0019_01         True\n",
       "3     0021_01         True\n",
       "4     0023_01         True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving predictions with their respective Ids from the test set\n",
    "predictions = pd.DataFrame(data = { 'PassengerId' : titanic_test['PassengerId'],\n",
    "                                    'Transported' : model_pred.astype(bool)               } )\n",
    "\n",
    "# checking the results\n",
    "predictions.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb557e69",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sending predictions to .csv file\n",
    "predictions.to_csv(path_or_buf = 'submission_Team_1.csv',\n",
    "                   index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57311367",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15.464924,
   "end_time": "2023-03-06T21:51:00.156934",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-06T21:50:44.692010",
   "version": "2.4.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
